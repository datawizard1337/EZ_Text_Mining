{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>EZ Text Mining with Python</center></h1>\n",
    "\n",
    "<center>Jan Kinne 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we will go through a typical text mining pipeline. Our goal is to train a machine learning model that classifies texts scraped from company website into two classes: \n",
    "- **0**: Text from a non-software company website.\n",
    "- **1**: Text from a software company website.\n",
    "\n",
    "Chapters:\n",
    "1. Load a labelled and an unlabelled dataset.\n",
    "2. Preprocess the texts.\n",
    "3. Vectorize the texts.\n",
    "4. Split the labelled dataset into a training set and a test set.\n",
    "5. Train a logit regression classifier.\n",
    "6. Calculate text similarities.\n",
    "7. Train a simple artifical neural network classifiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load [pandas](!https://pandas.pydata.org/), a Python package that allows us to load, manipulate, and analyse tabular data in Python. It is a very popular package and the de-facto standard for data scientists. We will shorten pandas name to \"pd\" because we will have to type it quiet often during our analysis. We will also set an option to enhance the rendering of outputs in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the pandas **read_csv()** function to read in the textfiles containing our data. The function will return an object (a so called \"pandas dataframe\") that contains our data in a table format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/labelled_data.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load \"labelled_data.txt\", a text file with \"labelled\" website data. \"Labelled\" means that each observation is already categorized as being part of a specific group or class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = pd.read_csv(\"labelled_data.txt\", sep=\"\\t\", encoding=\"utf-8\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use panda's **head()** method on our dataframe to show the first lines of our loaded labelled dataset. As we can see, our data is in a table format with 4 columns:\n",
    "- \"ID\": unique identifiers for each observation\n",
    "- \"url\" the website address from where text was downloaded\n",
    "- \"text\": the downloaded website text\n",
    "- \"software\": the label which tells us whether a website is from a software company (\"1\") or not (\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://autzen-reimers.de</td>\n",
       "      <td>Seite: « 1 / 0 » « 1 / 0 »AUTZEN &amp; REIMERSARCH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://ibos-goerlitz.de/</td>\n",
       "      <td>Das Ingenieurbüro IBOS GmbH wurde am 17.09.199...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://kaizhong-vogt.de/</td>\n",
       "      <td>capanne.gittinger.de</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://baecker-holland.de/</td>\n",
       "      <td>Klicken Sie hier um zu unserem Kon­takt­for­mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.vbhnr.de/privatkunden.html</td>\n",
       "      <td>Um Ihnen eine bessere Nutzung unserer Seite zu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                     url  \\\n",
       "0   0                http://autzen-reimers.de   \n",
       "1   1               https://ibos-goerlitz.de/   \n",
       "2   2               https://kaizhong-vogt.de/   \n",
       "3   3             https://baecker-holland.de/   \n",
       "4   4  https://www.vbhnr.de/privatkunden.html   \n",
       "\n",
       "                                                text  software  \n",
       "0  Seite: « 1 / 0 » « 1 / 0 »AUTZEN & REIMERSARCH...         0  \n",
       "1  Das Ingenieurbüro IBOS GmbH wurde am 17.09.199...         0  \n",
       "2                               capanne.gittinger.de         0  \n",
       "3  Klicken Sie hier um zu unserem Kon­takt­for­mu...         0  \n",
       "4  Um Ihnen eine bessere Nutzung unserer Seite zu...         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas **shape()** method on our dataframe to see how many rows and columns the dataframe has.\n",
    "\n",
    "The output tells us that we have 2,000 rows and 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the \"software\" column and check out how many \"1\"s (software companies) and \"0\"s (other company types) we have in our data. \n",
    "\n",
    "For that we select the \"software\" column by placing it in squared brackets behind our dataframe and then use pandas **value_counts()** method to count the ones and zeros in that column. As we can see, 1,716 \"0\"s and 284 \"1\"s are in the dataset, which means that we have many more non-software firms than software firms in our labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1716\n",
       "1     284\n",
       "Name: software, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data[\"software\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at our text data.\n",
    "\n",
    "First, we may want to see how large our texts are. We can do so by selecting the \"text\" column and use **apply(len)** on it. This will return us the lenght (number of characters) of each text in our dataset. By adding pandas **describe()** method, we will get some descriptive statistics telling us that the mean number number of characters (2,558.8) per website text, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2000.000000\n",
       "mean       2558.813500\n",
       "std        5144.796468\n",
       "min           1.000000\n",
       "25%         703.000000\n",
       "50%        1472.500000\n",
       "75%        2788.500000\n",
       "max      135504.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data[\"text\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding an additional statement in the format **DATA[DATA[\"COLUMN\"] == CONDITION]** we restrict the analysis to software firms only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      284.000000\n",
       "mean      3560.514085\n",
       "std       5080.710516\n",
       "min         11.000000\n",
       "25%       1171.250000\n",
       "50%       2393.000000\n",
       "75%       3916.500000\n",
       "max      57317.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data[labelled_data[\"software\"] == 1][\"text\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily plot the distribution of website text lengths by using pandas **plot()** method and passing the keywords for a histogram with 100 bins (**kind=\"hist, bins=100**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18ead51aac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labelled_data[\"text\"].apply(len).plot(kind=\"hist\", bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load unlabelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the same procedure as above to load a dataset with \"unlabelled\" data. In our case this means that the dataset contains website data without the \"software\" label, so we don't know whether the companies are software firms or not...but we will know soon by applying some machine learning magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://wimatec-mattes.de</td>\n",
       "      <td>Dieses Motto begleitet uns täglich bei der Arb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.jacobi.net/</td>\n",
       "      <td>Reactivated carbons for water and vapour treat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.basf.com/de.html</td>\n",
       "      <td>Cookies helfen uns bei der Bereitstellung unse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://gjb.de</td>\n",
       "      <td>Diese Seite verwendet Frames. Frames werden vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.roda-swiss.de/</td>\n",
       "      <td>| Transparenz und Ihre Privatsphäre sind uns w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                           url  \\\n",
       "0   0      http://wimatec-mattes.de   \n",
       "1   1       https://www.jacobi.net/   \n",
       "2   2  https://www.basf.com/de.html   \n",
       "3   3                 http://gjb.de   \n",
       "4   4    https://www.roda-swiss.de/   \n",
       "\n",
       "                                                text  \n",
       "0  Dieses Motto begleitet uns täglich bei der Arb...  \n",
       "1  Reactivated carbons for water and vapour treat...  \n",
       "2  Cookies helfen uns bei der Bereitstellung unse...  \n",
       "3  Diese Seite verwendet Frames. Frames werden vo...  \n",
       "4  | Transparenz und Ihre Privatsphäre sind uns w...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data = pd.read_csv(\"unlabelled_data.txt\", sep=\"\\t\", encoding=\"utf-8\", error_bad_lines=False)\n",
    "unlabelled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **shape** method, we see that we have 937 observations in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding short texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, there is quite a number of websites with texts that are shorter than 500 characters and some even had only a single character in their \"text\" column. We can exclude such websteis from our further analysis by overwriting the dataframe with a selection from the same dataframe restricted to websites with more than 499 characters in the text column.\n",
    "\n",
    "**shape** tells us that we now have 1,649 observations left in our labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1649, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = labelled_data[labelled_data[\"text\"].apply(len) > 499]\n",
    "labelled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same procedure should also be applied to the unlabelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data = unlabelled_data[unlabelled_data[\"text\"].apply(len) > 499]\n",
    "unlabelled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the texts in our dataset are exactly as they were downloaded from the company websites.\n",
    "\n",
    "Let's have a look at an example by displaying the observation with ID 697. We also alter a pandas option to display us more of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>697</td>\n",
       "      <td>http://www.eat-automation.de/</td>\n",
       "      <td>Planung, Projektierung, Realisierung, Service ____________________________________________________________________________________________________________ © 2008 - 2010 EAT GmbH | ProgrammierungWillkommen Deine Aufgaben - Montage von elektrotechnischen Anlagen (Schaltschrankbau, industrielle Elektroinstallation) - weltweite Inbetriebnahme - Programmierung von industriellen Steuerungssystemen (SIMATIC, Panasonic) Dein Profil - Real- oder Hauptschulabschluss - Gute Noten in den Fächern Mathematik, Physik - Teamfähigkeit, Eigenverantwortung und ein hohes Maß an Selbstständigkeit Ausbildungsbeginn: 08.2018 (Dauer 42 Monate) Interesse? Dann senden Sie Ihre vollständigen Bewerbungsunterlagen per Post oder E-Mail an uns.Wir wollen unser Team verstärken und suchen ab sofort eine / einen Auszubildenden / Automatisierungstechniker (w/m)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                            url  \\\n",
       "697  697  http://www.eat-automation.de/   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \\\n",
       "697  Planung, Projektierung, Realisierung, Service ____________________________________________________________________________________________________________ © 2008 - 2010 EAT GmbH | ProgrammierungWillkommen Deine Aufgaben - Montage von elektrotechnischen Anlagen (Schaltschrankbau, industrielle Elektroinstallation) - weltweite Inbetriebnahme - Programmierung von industriellen Steuerungssystemen (SIMATIC, Panasonic) Dein Profil - Real- oder Hauptschulabschluss - Gute Noten in den Fächern Mathematik, Physik - Teamfähigkeit, Eigenverantwortung und ein hohes Maß an Selbstständigkeit Ausbildungsbeginn: 08.2018 (Dauer 42 Monate) Interesse? Dann senden Sie Ihre vollständigen Bewerbungsunterlagen per Post oder E-Mail an uns.Wir wollen unser Team verstärken und suchen ab sofort eine / einen Auszubildenden / Automatisierungstechniker (w/m)   \n",
       "\n",
       "     software  \n",
       "697         0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "labelled_data[labelled_data[\"ID\"] == 697]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there can be quiet a lot of special characters and numbers in the text which we may want to exclude from our further analysis. We also may want to standardise all characters to lowercase, such that \"Software\" and \"software\" are recognized as the same words by the computer.\n",
    "\n",
    "We will import a python's \"regular expression\" operations and apply its **sub(\"FILTER\", \"REPLACE_STRING\")** function to the text column of our labelled dataset. We submit the sub() function with a regular expression telling it to delete all characters in the text that are not part of this list of characters: **\"abcdefghijklmnopqrstuvwxyzäöüß&. \"**. \n",
    "\n",
    "The method **lower()** will cast all characters to lowercase, while **strip()** will delete \"trailing\" whitespaces (e.g. \"last word    \" --> \"last word\").\n",
    "\n",
    "The results of this *operation* will be used to overwrite the orininal \"text\" column by using:\n",
    "\n",
    "labelled_data[\"text\"] = *operation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "labelled_data[\"text\"] = labelled_data[\"text\"].apply(lambda text: re.sub(\"[^abcdefghijklmnopqrstuvwxyzäöüß& ']\", \"\", str(text).lower()).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this step changed the text of our example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>software</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>697</td>\n",
       "      <td>http://www.eat-automation.de/</td>\n",
       "      <td>planung projektierung realisierung service      eat gmbh  programmierungwillkommen deine aufgaben  montage von elektrotechnischen anlagen schaltschrankbau industrielle elektroinstallation  weltweite inbetriebnahme  programmierung von industriellen steuerungssystemen simatic panasonic dein profil  real oder hauptschulabschluss  gute noten in den fächern mathematik physik  teamfähigkeit eigenverantwortung und ein hohes maß an selbstständigkeit ausbildungsbeginn  dauer  monate interesse dann senden sie ihre vollständigen bewerbungsunterlagen per post oder email an unswir wollen unser team verstärken und suchen ab sofort eine  einen auszubildenden  automatisierungstechniker wm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                            url  \\\n",
       "697  697  http://www.eat-automation.de/   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "697  planung projektierung realisierung service      eat gmbh  programmierungwillkommen deine aufgaben  montage von elektrotechnischen anlagen schaltschrankbau industrielle elektroinstallation  weltweite inbetriebnahme  programmierung von industriellen steuerungssystemen simatic panasonic dein profil  real oder hauptschulabschluss  gute noten in den fächern mathematik physik  teamfähigkeit eigenverantwortung und ein hohes maß an selbstständigkeit ausbildungsbeginn  dauer  monate interesse dann senden sie ihre vollständigen bewerbungsunterlagen per post oder email an unswir wollen unser team verstärken und suchen ab sofort eine  einen auszubildenden  automatisierungstechniker wm   \n",
       "\n",
       "     software  \n",
       "697         0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data[labelled_data[\"ID\"] == 697]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's apply the same operation on the text column of the unlabelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data[\"text\"] = unlabelled_data[\"text\"].apply(lambda text: re.sub(\"[^abcdefghijklmnopqrstuvwxyzäöüß&. ']\", \"\", str(text).lower()).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithms we will use require us to input our data in a numerical form. Raw text data as input will not work! This means that we have to transfer our texts to some kind of numerical representation without losing too much information. Transferring a text from a sequence of characters to a vector of numbers is called \"text vectorization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/text_vectorization.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to vectorize texts, from fancy techniques like [word embeddings](!https://en.wikipedia.org/wiki/Word_embedding) and topic models like [latent dirichlet allocation (LDA)](!https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to simple [word count models](!https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will keep it rather simple and use an approach called **TF-IDF** ([term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)).\n",
    "- **term frequency (TF)**: The number of times a term $t$ (word) appears in a document $d$ adjusted by the length of the document (number of all words $t'$ in document $d$).\n",
    "\n",
    "\\begin{equation*}\n",
    "TF(t, d) =   \\frac{f_t,_d}{\\sum{f_{t^\\prime},_d}}\n",
    "\\end{equation*}\n",
    "\n",
    "- **inverse document frequency (IDF)**: Counts the number of documents $n_t$ an individual term $t$ appears over all documents $N$.\n",
    "\\begin{equation*}\n",
    "IDF(t) =   log{\\frac{N}{1 + n_t}}\n",
    "\\end{equation*}\n",
    "\n",
    "- **term frequency-inverse document frequency (TFIDF)**: This step weights down common words like \"the\" and gives more weight to rarer words like \"software\".\n",
    "\n",
    "\\begin{equation*}\n",
    "TFIDF(t, d) = TF(t, d) * IDF(t)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use scikit-learn' [TF-IDF Vectorizer](!https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to generate TF-IDF vectors from our texts. Scikit-learn is the most popular machine learning package for Python and includes all kinds of ML algorithms for stuff like classification and clustering, but also handy preprocessing tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/sklearn.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating TF-IDF vectors from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: In this tutorial, we fit our TF-IDF vectorizer on our entire labelled dataset. In real life, you should instead fit your vectorizer on the \"training set\" part of you labelled dataset only.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to initialize a TF-IDF vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to teach our vocabulary to the vectorizer. For that we can use the vectorizer's **fit()** method on our texts. In this step, the vectorizer also calculates the inverse document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vectorizer = vectorizer.fit(labelled_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the trained vectorizer's **transform()** method to transform any text document to a TF-IDF vector.\n",
    "\n",
    "Let's transform the sentence \"dies ist ein Test\" and output the resulting vector using Python's **print()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 64619)\t0.7674788380733668\n",
      "  (0, 34005)\t0.22768716559203767\n",
      "  (0, 16620)\t0.2383998438323033\n",
      "  (0, 14515)\t0.5498184265691286\n"
     ]
    }
   ],
   "source": [
    "example_tfidf = trained_vectorizer.transform([\"dies ist ein test\"])\n",
    "print(example_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output you see is a so-called sparse matrix. In a sparse matrix, only non-zero elements are memorized and mapped using indexes. This actually saves A LOT of memory. In the example above, there are only four non-zero elements in the matrix and their coordinates/indexes are given in the left parantheses. The elements on the right hand side give you the corresponding TF-IDF value for the word mapped by the coordinates.\n",
    "\n",
    "Let's have a look at the vocabulary the vectorizer learned from our data. We call the **vocabulary_** method on the trained vectorizer to retrieve the full vocabulary and the use a little loop to print the first items in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('das', 13209)\n",
      "('ingenieurbüro', 32913)\n",
      "('ibos', 31714)\n",
      "('gmbh', 27194)\n",
      "('wurde', 74328)\n",
      "('am', 1890)\n",
      "('in', 32182)\n",
      "('görlitz', 28454)\n",
      "('gegründet', 24982)\n",
      "('unser', 67469)\n",
      "('handlungsschwerpunkt', 28901)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = trained_vectorizer.vocabulary_\n",
    "\n",
    "#little loop to print the first items in the vocabulary\n",
    "for count, item in enumerate(iter(vocabulary.items())):\n",
    "    print(item)\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve the vocabulary index of a word directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64619"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how many words are in our vocabulary, actually? We can see that by calculating its length using Python's **len()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76713"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are quite a lot of words. It may be a good idea to shrink down our vocabulary a bit, especially because this will reduce both memory consumption and required computational power when we start doing ML magic!\n",
    "\n",
    "A common approach to do so is to apply so-called popularity-based filtering. Hereby, we exclude very common and/or extremly uncommon words from our vocabulary. This can be achieved by passing the corresponding parameters to the vectorizer when we initialize it. \n",
    "\n",
    "Let's overwrite our vectorizer and create a new one which includes words that appear in at least 1% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', min_df=0.01)\n",
    "trained_vectorizer = vectorizer.fit(labelled_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary should be way smaller now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2628"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = trained_vectorizer.vocabulary_\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split our labelled dataset into a **training set** and a **test set**. The training set will be used to train our machine learning model to predict the correct labels (classes) of the observations in the training set based on their corresponding texts. After training, we will use the trained model to predict the labels of all the observations in the test set (which was not used for training). Based on the prediction performance in the test set, we can evaluate our trained model.\n",
    "\n",
    "This two-step approach is used to make sure that the ML model does not simply memorize all the observations in the training data, but instead derive universal rules or patterns to distinguish the different classes. This ability is called **generalization** and it is very desireable in machine learning. In contrast, the overspecialization on the training set and a model's resulting bad performance using other data is called **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/training_split.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **training** of a machine learning model describes the process of teaching the model how to achive a certain learning task. In the case of classification tasks, we give the model a list of properties $X$ (**features** or **attributes**) that are used to calculate a predicted outcome (label/class) $\\hat{Y}$. We then compare the predicted outcome $\\hat{Y}$ to the true outcome $Y$ that we know because we have a labelled dataset. The difference between the predicted and the true outcome is then used to calculate an **error**. We then start to **optimize** (train) the model by adjusting its internal numbers $W$ (**weights**) to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/training.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the features (attributes) of our observations are the texts represented as numerical TFIDF vectors and our labels are the \"1\" and \"0\" classes in the \"software\" column. \n",
    "\n",
    "So let's first shuffle our data and then create a list with our features $X$ and a list with the corresponding labels $Y$. For the features we select the text column from our labelled dataset and transform them to TFIDF vectors using our trained vactorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = labelled_data.sample(frac=1.0, random_state=12)\n",
    "features = trained_vectorizer.transform(labelled_data[\"text\"])\n",
    "labels = labelled_data[\"software\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber that we hat 1,649 observations in our labelled dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1649, 4)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first 1,250 observations for the training set. The remaining obseravtions will be assigned to the test set and put aside for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_trainset = features[:1250]\n",
    "labels_trainset = labels[:1250]\n",
    "\n",
    "features_testset = features[1250:]\n",
    "labels_testset = labels[1250:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in our training set are stored in a sparse matrix format with dimension 1250 (the number of our training samples) by 2628 (the size of our vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1250x2628 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 141525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels in our training set are integers stored in pandas Series (a single-column dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305     0\n",
       "1413    0\n",
       "1954    0\n",
       "483     0\n",
       "115     1\n",
       "       ..\n",
       "1303    0\n",
       "1970    0\n",
       "1779    1\n",
       "425     0\n",
       "353     0\n",
       "Name: software, Length: 1250, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training a logit regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our first model. We will start with something basic: A [logistic regression classifier](https://en.wikipedia.org/wiki/Logistic_regression). This classifier is pretty popular model for binary outcome variables. Again, we will use scikit-learn for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to initialize the logisitic regression. We pass it the parameter **(class_weight=\"balanced\")** because we have a pretty unbalanced dataset (one class in way more frequent than the other). The \"balanced\" parameter will make sure that the model will pay more attention to the infrequent class (in our case the software = 1 class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_classifier = LogisticRegression(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the classifier using its **fit()** method and passing the features and corresponding labels of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_logit_classifier = logit_classifier.fit(features_trainset, labels_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just trained your first machine learning model! But how good is it at distinguishing the web texts of software firms from other firm types? \n",
    "\n",
    "Let's test that with an example sentence that we transfer to a TFIDF vector using our trained vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bauernhof_tfidf = trained_vectorizer.transform([\"das ist ein bauernhof\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass this TFIDF vector to our trained logit classifier and tell it to predict its label using the **predict()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_logit_classifier.predict(bauernhof_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted label is \"0\" (aka \"not a software company\"). Awesome!\n",
    "\n",
    "We can also check out the probability for both classes by using the **predict_proba()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64401239, 0.35598761]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_logit_classifier.predict_proba(bauernhof_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number above gives you the probability that the label is \"0\" while the second number is the probability that the label is \"1\". So the classifier is not too confident (about 63%) that the text comes from the website of a non-software company (**WARNING: Your results may differ!**).\n",
    "\n",
    "Let's try one more example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04328043, 0.95671957]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programmer_tfidf = trained_vectorizer.transform([\"wir programmieren software\"])\n",
    "trained_logit_classifier.predict_proba(programmer_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! In this example, the model is absolutely sure (96%) that the text comes from a sofware company.\n",
    "\n",
    "We should now test our classifier using the test set which we put aside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = trained_logit_classifier.predict(features_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now **print()** the predicted labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
      " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the true labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels_testset.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and compare them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False  True False  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False  True\n",
      "  True  True  True  True  True  True  True False  True False  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True False  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True False  True  True  True False  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True  True  True  True  True  True  True  True False  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True False  True False  True\n",
      " False  True  True  True False  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True False  True  True False  True False\n",
      "  True  True  True  True  True False False  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels == labels_testset.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow...at first glace that looks pretty convincing. It seems like most of the predicted labels match their true counterparts, but not all of them.\n",
    "\n",
    "Let's quantify the classifier's prediction performance by generating a scikit-learn **classification report**. The report contains several measures that allow us to evaluate the performance of our trained model in the test set:\n",
    "\n",
    "- **precision**: the fraction of observations that were predicted to have label $\\hat{y} = 1$ and that actually have the true label $y = 1$.\n",
    "- **recall**: the fraction of observations that have the true label $y = 1$ and that were predicted to have $\\hat{y} = 1$.\n",
    "- **f1-score**: a composite measure that combines both precision and recall.\n",
    "- **support**: simply the number of observations with this true label in the test set.\n",
    "\n",
    "<img src=\"pics/classification_report.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93       323\n",
      "           1       0.69      0.78      0.73        76\n",
      "\n",
      "    accuracy                           0.89       399\n",
      "   macro avg       0.82      0.85      0.83       399\n",
      "weighted avg       0.90      0.89      0.89       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! As you can see, the classification report gives you precision, recall, f1-score, and support for both labels (\"1\" and \"0\"). We could summarize the report as follows:\n",
    "- 95% of the observations that were labeled \"0\" by the classifier actually have the true label \"0\". For label \"1\" this value is only 69%\n",
    "- 92% of the observations that have the true label \"0\" were also predicted to have have the label \"0\" by the classifier. For label \"1\" this value is only 78%.\n",
    "\n",
    "So if our goal was to identify most of the software firms in the unlabelled dataset (**true positives**) while limiting the number of non-software firms that are wrongly classified as software firm (**false positives**), we could say:\n",
    "\"7 of 10 firms that were predicted to be software firms are actually software firms and we are able to recover 8 of 10 software firms in the dataset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we can create a new column with our predicted labels in our labelled dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data.loc[labelled_data.index[1250]:, 'prediction'] = predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and have a look at some observations that were incorrectly predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data[(labelled_data[\"software\"] != labelled_data[\"prediction\"]) & (labelled_data[\"prediction\"].notnull())].sample(2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we create a new column \"software\" in our unlabelled dataset and predict the labels using our trained logit classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data[\"software\"] = unlabelled_data[\"text\"].apply(lambda text: trained_logit_classifier.predict(trained_vectorizer.transform([text]))[0])\n",
    "unlabelled_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calculating text similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing texts as vectors has another advantage: It allows us to easily calculate distances between pairs of texts. By calculating such pairwise distances, we are able to identify pairs of texts that are very similar (close) or dissimilar (distant) to each other. Calculating the distances between observations in our dataset can be understood as comparing the business models of the companies as they are described on their websites. Let's try this out and see whether we can find GIS software companies in our unlabelled dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to retrain our TFIDF vectorizer on our unlabelled dataset without the popularity-based filtering. This makes sure that all the words (41,955) in the unlabelled dataset are included in the resulting vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word')\n",
    "trained_vectorizer = vectorizer.fit(unlabelled_data[\"text\"])\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a single TFIDF vector from a description of the company type we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tfidf = trained_vectorizer.transform([\"gis geographie geographische informationssysteme geodaten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the popular [**cosine similary**](!https://www.machinelearningplus.com/nlp/cosine-similarity/) to calculate the similarity between our TFIDF vectors. Cosine similarity measures the similarity between vectors based on their orientation in the high-dimensional vector space. The smaller the angle between two documents, the more similar they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will use a function from scikit-learn [**cosine_similarity()**](!https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to calculate the cosine similarity between our search TFIDF vectors and the TFIDF vectors representing the company website texts in our unlabelled dataset. We do so by applying the function to the text column and creating a new column \"similarity\" with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "unlabelled_data[\"similarity\"] = unlabelled_data[\"text\"].apply(lambda text: cosine_similarity(search_tfidf, trained_vectorizer.transform([text])).tolist()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at those software firms that are most similar to our search vector. We can do that by restricting our dataframe to software firms and sorting it (**sort_values()**) by our new \"similarity\" columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data[unlabelled_data[\"software\"] == 1].sort_values(by=[\"similarity\"], ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our next classifier, we will use one of those fancy artifical neural networks (ANN) everyone is talking about. ANNs consist of several **layers** of so-called **neurons** (or **nodes**) which are linked with all the neurons in the next layer. These connections are called **edges** and have weights attached to them which control the strength of the connection between two neurons. These edges are adjusted during the training process.\n",
    "\n",
    "In "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/ANN.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer consists of only a single neuron that takes all the outputs from the previous layer as input and squeezes them to a number between 0 and 1 using a so-called **sigmoid activation function**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/sigmoid.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our ANN we will use [Keras](!https://keras.io/), a Python deep learning library. From Keras, we need to import some modules first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct our simple ANN by defining its layers and combine them in a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 2628\n",
    "\n",
    "# The input layer with a number of neurons equal to the size of our vocabulary\n",
    "input_layer = layers.Input(shape=(vocabulary_size,))\n",
    "\n",
    "# The only hidden layer in our simple AAN, consisting of 25 neurons that take in the input_layer's outputs \n",
    "hidden_layer = layers.Dense(25)(input_layer)\n",
    "\n",
    "# The output layer which consists of only a single neuron with a sigmoid activation function\n",
    "# The input for this layer are the outputs of the hidden_layer\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "# Initialize the model using Keras model() function and passing it the input and output layers\n",
    "simple_ANN = models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we compile the model and prepare it for training by defining the:\n",
    "- **optimizer**: The algorithm we want to use to adjust the weights in our ANN and find their optimum setting during the training.\n",
    "- **learning rate (lr)**: A parameter that controls how vigorously we change the weights in our ANN during each training step. If we are too vigorous, we will overshoot and if we are too reluctant it will take ages to find the optimum weights.\n",
    "- **loss**: The loss function (also called cost, error, or objective function) we want to use to determine how far off our predictions are from the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ANN.compile(optimizer=optimizers.Adam(lr=0.005), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also get a summary of our model using the **summary()** function. As you can see, there are 65,751 trainable parameters in our model. So, let's train them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ANN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we **fit()** the model on our training set. We also need to define:\n",
    "- **epochs**: The number of full iterations through all the observations in the training data. If we adjust the weights only very reluctant (i.e. small learning rate), we are likely to need more epochs.\n",
    "- **validation_split**: The share of observations from the training data that will be taken aside as the **validation set**. The validation set is like an additional test set which will not be used for training the neural network.\n",
    "\n",
    "After each epoch, the loss in both the training data and the validation data is reported. This allows us to monitor the training process and whether the ANN is actually learning something (i.e. decreasing the loss). Ideally, the performance in both the training and the validation data should improve. If the performance in the validation data stagnates while it is still improving in the training data, we are likely to overfit our model to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ANN.fit(features_trainset, labels_trainset, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, we can use our simple ANN to predict the labels for our testset observation using the **predict()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = simple_ANN.predict(features_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because predict() will return us the probability (e.g. 0.61) that a observation has the label \"1\", we need to round the predictions using numpy's **round()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_labels = np.round(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can finally print out a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
